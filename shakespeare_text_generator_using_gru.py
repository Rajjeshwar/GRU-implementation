# -*- coding: utf-8 -*-
"""Shakespeare_text_generator_using_GRU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uNHvl_u5UjVHeSM2q4Y6Vq4WlzF9UYMw

# Import modules
"""

import tensorflow as tf
import string
import requests
import numpy as np

import matplotlib.pyplot as plt

import sklearn as sk
from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.initializers import GlorotNormal
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Dropout

"""# Prepare data"""


def scrape_text_data():
    response = requests.get(
        "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
    )
    return response


response = scrape_text_data()

response.text

type(response.text)

"""In this step, we get rid of the `\n` qualifier, and join it using separate lines."""


def format_text(response):
    data = response.text.split("\n")[0:]
    data = "".join(data)
    return data


data = format_text(response)

len(data)

data

data[1]


def filter_text(corpus):
    token = corpus.split()
    table = str.maketrans("", "", string.punctuation)
    token = [word.translate(table) for word in token]
    # saving only alphanumeric
    token = [word for word in token if word.isalpha()]
    token = [word.lower() for word in token]
    return token


tokens = filter_text(data)

"""## Visualize tokens

"""

print(tokens[:20])

len(tokens)

# calclate vector size from corpus
len(set(tokens))

# set length of seed and number of lines to use for training
def set_seed_length(tokens):
    seed_sequence_length = 50
    lines = []
    for i in range(seed_sequence_length + 1, len(tokens)):
        seq = tokens[i - (seed_sequence_length + 1) : i]
        line = " ".join(seq)
        lines.append(line)
        if i > 2 * 10 ** 5:
            break
    return lines, seed_sequence_length


lines, seed_sequence_length = set_seed_length(tokens)

print(len(lines))

len(lines[0])

"""# Preprocessing """


def vocabulary_tokenizer(lines):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(lines)
    sequences = tokenizer.texts_to_sequences(lines)
    return sequences, tokenizer


sequences, tokenizer = vocabulary_tokenizer(lines)

sequences[1]

tokenizer.word_index

len(tokenizer.word_index)

vocabulary_length = len(tokenizer.word_index) + 1  # 0 indexed

print(len(sequences))


def create_train_test(sequences, vocabulary_length):
    sequences = np.array(sequences)
    X, Y = sequences[:, :-1], sequences[:, -1]
    Y = to_categorical(Y, vocabulary_length)
    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)
    X_val, X_test, Y_val, Y_test = train_test_split(X_val, Y_val, test_size=0.5)
    return X_train, X_val, X_test, Y_train, Y_val, Y_test


X_train, X_val, X_test, Y_train, Y_val, Y_test = create_train_test(
    sequences, vocabulary_length
)

"""# Model"""


def GRU_model():
    model = Sequential()
    model.add(Embedding(vocabulary_length, 50, input_length=X_train.shape[-1]))
    model.add(
        GRU(
            128,
            dropout=0.2,
            unroll=False,
        )
    )
    model.add(Dense(vocabulary_length, activation="softmax"))
    return model


model = GRU_model()

model.summary()

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

"""# Train"""

history = model.fit(X_train, Y_train, batch_size=64, epochs=100)


def calc_perplexity():
    train_loss = history.history["loss"]
    train_perplexity = tf.exp(train_loss)
    return train_perplexity


def plot_metrics():
    plt.plot(history.history["loss"])
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend(["loss"])


plot_metrics()

train_perplexity = calc_perplexity()
plt.plot(train_perplexity)
plt.xlabel("Epochs")
plt.ylabel("Perplexitiy")
plt.legend(["perplexity"])

np.max(train_perplexity), np.min(train_perplexity)

"""## Text Generation

"""

seed = lines[12343]

print(seed)


def generate_text(model, tokenizer, seed_sequence_length, seed_text, n_words):
    text = []

    for _ in range(n_words):
        encoded = tokenizer.texts_to_sequences([seed_text])[0]
        encoded = pad_sequences(
            [encoded], maxlen=seed_sequence_length, truncating="pre"
        )

        y_predict = model.predict(encoded)
        y_predict = y_predict.argmax(axis=1)

        predicted_word = ""
        for word, index in tokenizer.word_index.items():

            if index == y_predict:
                predicted_word = word
                break
        seed_text = seed_text + " " + predicted_word
        text.append(predicted_word)
    return " ".join(text)


generate_text(model, tokenizer, seed_sequence_length, seed, 100)
